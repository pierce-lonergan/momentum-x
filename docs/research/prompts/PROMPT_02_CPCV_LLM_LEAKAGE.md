# RESEARCH PROMPT 2: Purged & Embargoed Cross-Validation with LLM Temporal Leakage
# Target: docs/research/CPCV_LLM_LEAKAGE.md → src/core/backtester.py enhancement
# Priority: CRITICAL — Backtest integrity is foundational (TOR-P Invariant #1)

## RESEARCH QUERY

I am building an institutional-grade backtesting framework for a multi-agent LLM
trading system (MOMENTUM-X). The system uses 6 LLM-powered agents that analyze
news sentiment, technical patterns, fundamental data, institutional flows, SEC
filings, and risk factors to generate trading signals for explosive momentum stocks
(+20% daily moves).

My backtester implements Purged and Embargoed Cross-Validation (CPCV) per
de Prado (2018), but I have identified a novel leakage vector: **LLM temporal
contamination**. The LLMs powering my agents were pretrained on data that includes
historical market events, news articles, and financial outcomes up to their
knowledge cutoff. This means:

- When backtesting a trade from 2023-06-15, the News Agent (powered by GPT-4/Claude)
  may "know" what happened after that date from pretraining data.
- Standard CPCV purges overlapping time windows between train/test folds, but
  cannot purge information embedded in the LLM's weights.
- This creates inflated backtest performance that doesn't generalize to live trading.

### SPECIFIC QUESTIONS TO RESEARCH

1. **Characterizing LLM Temporal Leakage**
   - What is the formal definition of "temporal leakage" in the context of
     LLM-powered trading systems?
   - How does this differ from classic look-ahead bias in time-series CV?
   - Can we quantify the information advantage an LLM has when analyzing
     historical events it was trained on vs. truly novel events?
   - Reference: de Prado (2018) "Advances in Financial ML", Ch. 7 on CPCV.

2. **Mitigation Strategies**
   - **Strategy A: Temporal Prompt Engineering** — Force the LLM to role-play as
     if it's operating on the backtest date. E.g., "You are analyzing this stock
     on 2023-06-15. You have NO knowledge of events after this date."
     How effective is this? Research on LLM instruction-following for temporal
     constraints.
   - **Strategy B: Knowledge Cutoff Alignment** — Only backtest periods AFTER
     the LLM's knowledge cutoff. E.g., if using Claude with April 2024 cutoff,
     only backtest June 2024+. This is clean but limits historical data.
   - **Strategy C: Dual-Track Backtesting** — Run backtests with a "dumb" agent
     (rules-based, no LLM) and an LLM agent. Compare the performance differential
     on pre-cutoff vs. post-cutoff data. The pre-cutoff differential estimates
     the LLM leakage premium.
   - **Strategy D: Synthetic Event Injection** — Replace real news/events in
     historical backtests with synthetic events generated by a separate LLM.
     This eliminates memorization but changes the data distribution.
   - Reference: Lopez de Prado (2023) on ML in finance, Grinsztajn et al. (2022)
     on tree-based vs. neural approaches.

3. **Modified CPCV for LLM Pipelines**
   - How should the purge window be extended to account for LLM temporal
     leakage? Is there a principled way to estimate the "information radius"
     of an LLM's pretraining on a given event?
   - Can we implement "LLM-aware embargo" — an extended embargo period that
     accounts for the LLM's knowledge window?
   - What is the impact on effective sample size when the embargo is extended
     significantly?
   - Reference: de Prado (2018) Ch. 7.4.3 (embargo sizing).

4. **Calibration and Detection**
   - How can we detect if LLM leakage is occurring in a given backtest?
   - Proposed test: Compare LLM agent accuracy on events before vs. after
     the knowledge cutoff. A statistically significant difference indicates
     temporal contamination.
   - What statistical tests are appropriate? (Permutation test, Wilcoxon
     rank-sum, bootstrap confidence intervals?)
   - Reference: Bailey et al. (2014) probability of backtest overfitting.

5. **Combinatorial Purged Cross-Validation Details**
   - My current CPCV uses k=5 groups and k-1 test combinations.
     What is the optimal k for a 2-year daily trading dataset (~500 samples)?
   - How do I handle the fact that explosive momentum trades are rare events
     (maybe 5-10% of days have qualifying candidates)?
   - Should I use stratified sampling to ensure each fold has similar
     proportions of momentum events?
   - Reference: de Prado (2018) Ch. 12 (backtesting on synthetic data),
     Molnar et al. (2024) on proper evaluation of time-series models.

6. **Practical Architecture**
   - How should the backtester serialize agent responses for replay?
     (Cache LLM outputs for deterministic backtest reruns)
   - What is the storage requirement for caching 6 agents × 500 days × N candidates?
   - How to handle the non-determinism of LLM outputs across reruns?
   - Should backtest use temperature=0 for deterministic replay?

### CONSTRAINTS
- Backtester must satisfy TOR-P Invariant #1: CPCV is mandatory
- Must work with multiple LLM providers (OpenAI, Anthropic, DeepSeek)
- Target dataset: 2 years of daily US equities, focus on +20% gap stocks
- Must produce: deflated Sharpe ratio, probability of backtest overfitting (PBO)
- Python implementation, property-testable invariants

### DELIVERABLE FORMAT
Research summary in Markdown with:
- Formal taxonomy of LLM leakage types (temporal, distributional, structural)
- Decision matrix: Strategy A/B/C/D trade-offs (cost, validity, data requirements)
- Modified CPCV algorithm pseudocode with LLM-aware embargo
- Leakage detection test specification (statistical test + threshold)
- ADR draft for "LLM-Aware Backtesting" approach selection
- Math formalization for docs/mathematics/MOMENTUM_LOGIC.md §17
- Bibliography of 10-15 papers (de Prado, Bailey, arXiv ML-finance papers)
